{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pathlib\n","import pandas as pd\n","\n","datadir = pathlib.Path.cwd().parent / \"data\"\n","\n","pd_aozora = pd.read_parquet(datadir / \"aozora.parquet\")"]},{"cell_type":"markdown","id":"4a4f8313","metadata":{},"source":["# 13章 文字列\n","## 13-1 部分文字列の検出・抽出・置換\n","### Q: URLから芥川竜之介の作品を判定して抽出\n"]},{"cell_type":"markdown","id":"909dee0e","metadata":{},"source":["#### Awesome"]},{"cell_type":"code","execution_count":null,"id":"8c6d07a1","metadata":{},"outputs":[],"source":["pd_aozora.loc[lambda df: df.url.str.contains(\"/000879/\")]"]},{"cell_type":"markdown","id":"70c4a4eb","metadata":{},"source":["### Q: 出版年月から年と月を位置指定で抽出\n"]},{"cell_type":"markdown","id":"5bf248de","metadata":{},"source":["#### Awesome"]},{"cell_type":"code","execution_count":null,"id":"34103737","metadata":{},"outputs":[],"source":["(\n","    pd_aozora\n","    .assign(\n","        # published_atの文字列の一部を抽出\n","        year=lambda df: df.published_at.str[:4],\n","        month=lambda df: df.published_at.str[5:7]\n","    )\n","    [[\"text_id\", \"title\", \"published_at\", \"year\", \"month\"]]\n",")"]},{"cell_type":"markdown","id":"9a3ccebe","metadata":{},"source":["### Q: 出版年月のフォーマットを`YYYY年MM月`から`YYYY-MM`に変換（部分文字列の検出、置換）\n"]},{"cell_type":"markdown","id":"982fd16c","metadata":{},"source":["#### Awesome"]},{"cell_type":"code","execution_count":null,"id":"f4a051df","metadata":{},"outputs":[],"source":["import numpy as np\n","\n","(\n","    pd_aozora\n","    .assign(published_at=lambda df:\n","        # published_atが\"月\"を含む場合は\"年\"を\"-\"に置換して\"月\"を削除、それ以外は\"年\"を削除\n","        np.where(df.published_at.str.contains(\"月\"),\n","            df.published_at.str.replace(\"年\", \"-\").str.replace(\"月\", \"\"),\n","            df.published_at.str.replace(\"年\", \"\"))\n","    )\n","    [[\"text_id\", \"title\", \"published_at\"]]\n",")"]},{"cell_type":"markdown","id":"be668da4","metadata":{},"source":["### Q: URLから人物IDと作品IDを抽出 (正規表現による抽出)\n"]},{"cell_type":"markdown","id":"99256912","metadata":{},"source":["#### Awesome"]},{"cell_type":"code","execution_count":null,"id":"a219ca31","metadata":{},"outputs":[],"source":["(\n","    pd_aozora\n","    .assign(\n","        # urlから正規表現で抽出\n","        author_id=lambda df: df.url.str.extract(r\"cards/0*([1-9][0-9]*)/\"),\n","        text_id=lambda df: df.url.str.extract(r\"card([0-9]+)\\.\")\n","    )\n","    [[\"title\", \"url\", \"author_id\", \"text_id\"]]\n",")"]},{"cell_type":"markdown","id":"484ab807","metadata":{},"source":["## 13-2 区切り文字による文字列分割\n","### Q: 句点で文に分割\n"]},{"cell_type":"markdown","id":"c23a4220","metadata":{},"source":["#### Awesome"]},{"cell_type":"code","execution_count":null,"id":"58cfdf73","metadata":{},"outputs":[],"source":["(\n","    pd_aozora\n","    # \"。\"でtextを分割\n","    .assign(sentence_list=lambda df: df.text.str.split(\"。\"))\n","    [[\"text_id\", \"title\", \"sentence_list\"]]\n",")"]},{"cell_type":"markdown","id":"1d13582c","metadata":{},"source":["### Q: 複数の区切り文字で文に分割\n"]},{"cell_type":"markdown","id":"eb9e7128","metadata":{},"source":["#### Awesome"]},{"cell_type":"code","execution_count":null,"id":"f5aee1b2","metadata":{},"outputs":[],"source":["(\n","    pd_aozora\n","    # \"。\"または\"\\n\"でtextを分割\n","    .assign(sentence_list=lambda df: df.text.str.split(r\"。|\\n\"))\n","    [[\"text_id\", \"title\", \"sentence_list\"]]\n",")"]},{"cell_type":"markdown","id":"e7fcdbed","metadata":{},"source":["### Q: 分割結果の配列を列に展開\n"]},{"cell_type":"markdown","id":"5abb25b4","metadata":{},"source":["#### Awesome"]},{"cell_type":"code","execution_count":null,"id":"5a5b2544","metadata":{},"outputs":[],"source":["(\n","    pd_aozora\n","    .assign(\n","        #（1）\"。\"でtextを分割\n","        sentence_list=lambda df: df.text.str.split(\"。\"),\n","        #（2）sentence_listの0番目と1番目の要素を取得\n","        first_sentence=lambda df: df.sentence_list.str[0],\n","        second_sentence=lambda df: df.sentence_list.str[1]\n","    )\n","    [[\"text_id\", \"title\", \"first_sentence\", \"second_sentence\"]]\n",")"]},{"cell_type":"markdown","id":"4d315ecc","metadata":{},"source":["### Q: 分割結果の配列を行に展開\n"]},{"cell_type":"markdown","id":"836b8c56","metadata":{},"source":["#### Awesome"]},{"cell_type":"code","execution_count":null,"id":"35408b9a","metadata":{},"outputs":[],"source":["(\n","    pd_aozora\n","    #（1）\"。\"でtextを分割\n","    .assign(sentence=lambda df: df.text.str.split(\"。\"))\n","    [[\"text_id\", \"title\", \"sentence\"]]\n","    #（2）sentenceを複数行に展開\n","    .explode(\"sentence\")\n",")"]},{"cell_type":"markdown","id":"4331da45","metadata":{},"source":["## 13-3 文字列データのクレンジング\n","### Q: ユニコード正規化\n"]},{"cell_type":"markdown","id":"a4d8dc7b","metadata":{},"source":["#### Awesome"]},{"cell_type":"code","execution_count":null,"id":"b5402bd1","metadata":{},"outputs":[],"source":["(\n","    pd_aozora\n","    # textをユニコード正規化\n","    .assign(text=lambda df: df.text.str.normalize(\"NFKC\"))\n","    [[\"text_id\", \"title\", \"text\"]]\n",")"]},{"cell_type":"markdown","id":"44ca0385","metadata":{},"source":["### Q: 不要な文字列パターンの除去（正規表現による除去）\n"]},{"cell_type":"markdown","id":"2aca9e2c","metadata":{},"source":["#### Awesome"]},{"cell_type":"code","execution_count":null,"id":"4a3aa4db","metadata":{},"outputs":[],"source":["pd_aozora_after_cleansing = (\n","    pd_aozora\n","    # （1） 文への分割と行展開\n","    .assign(sentence=lambda df: df.text.str.split(\"。|\\n\"))\n","    [[\"text_id\", \"title\", \"sentence\"]]\n","    .explode(\"sentence\") \n","    .assign(\n","        sentence=lambda df:\n","            df\n","            # （2） ユニコード正規化\n","            .sentence.str.normalize(\"NFKC\")\n","            # （3） 正規表現による不要パターンの除去\n","            .str.replace(r\"《.*?》|\\[#.*?\\]|[\\|※「」()]\", \"\", regex=True)\n","            # （4） 先頭や末尾のスペースの除去\n","            .str.strip()\n","    )\n","    # （5） 長さが1文字以上の行のみ抽出\n","    .query(\"sentence.str.len() >= 1\")\n",")\n","pd_aozora_after_cleansing\n"]},{"cell_type":"markdown","id":"74edacb8","metadata":{},"source":["## 13-4 形態素解析による日本語文章の単語分解と単語抽出\n","### Q: 文を単語に分解（形態素解析）\n","#### Awesome"]},{"cell_type":"code","execution_count":null,"id":"450277c3","metadata":{},"outputs":[],"source":["# spaCyおよびGiNZAモジュールの読み込み\n","# 初回はpipでインストールする\n","# pip install spacy ja-ginza\n","import spacy\n","nlp = spacy.load(\"ja_ginza\")\n","\n","pd_aozora_token = (\n","    pd_aozora_after_cleansing\n","    # （1） 単語（Tokenオブジェクト）への分解と行展開\n","    .assign(token=lambda df: list(nlp.pipe(df.sentence)))\n","    [[\"text_id\", \"token\"]]\n","    .explode(\"token\")\n","    # （2） Tokenオブジェクトのプロパティから列を作成\n","    .assign(\n","        # 元の文章での表記\n","        text=lambda df: df.token.apply(lambda e: e.text),\n","        # 単語の原型\n","        lemma=lambda df: df.token.apply(lambda e: e.lemma_),\n","        # 単語の品詞\n","        pos=lambda df: df.token.apply(lambda e: e.pos_),\n","        # 単語がストップワードかどうかを表すブール値\n","        is_stop=lambda df: df.token.apply(lambda e: e.is_stop),\n","        # 単語にベクトル表現が付与されているかどうかを表すブール値\n","        has_vector=lambda df: df.token.apply(lambda e: e.has_vector),\n","        # 単語のベクトル表現 （付与されていない場合はゼロベクトル)\n","        vector=lambda df: df.token.apply(lambda e: e.vector)\n","    )\n","    # （3） 不要な列の削除、Index修正\n","    .drop(columns=\"token\")\n","    .reset_index(drop=True)\n",")\n","pd_aozora_token"]},{"cell_type":"markdown","id":"b3853cc2","metadata":{},"source":["### Q: 特定の品詞の抽出とストップワードの除去\n","#### Awesome"]},{"cell_type":"code","execution_count":null,"id":"c52dff70","metadata":{},"outputs":[],"source":["# 名詞と固有名詞の抽出とストップワードの除去\n","pd_aozora_token.query(\"pos in ('NOUN', 'PROPN') and not is_stop\")"]},{"cell_type":"markdown","id":"78b9ddb6","metadata":{},"source":["## 13-5 文章のベクトル化\n","### Q: bag-of-wordsベクトルの作成\n","#### Awesome"]},{"cell_type":"code","execution_count":null,"id":"cc953bfd","metadata":{},"outputs":[],"source":["(\n","    pd_aozora_token\n","    # （1） 名詞と固有名詞の抽出とストップワードの除去\n","    .query(\"pos in ('NOUN', 'PROPN') and not is_stop\")\n","    # （2） text_idごとにlemmaの値をカウントし、横持ち行列に変換\n","    .groupby([\"text_id\", \"lemma\"]).size()\n","    .unstack()\n",")"]},{"cell_type":"code","execution_count":null,"id":"c92f80b5","metadata":{},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# text_idごとの単語リストを作成\n","pd_word_list = (\n","    pd_aozora_token\n","    .query(\"pos in ('NOUN', 'PROPN') and not is_stop\")\n","    # text_idごとにlemmaのlistを作成\n","    .groupby(\"text_id\").lemma.apply(list)\n",")\n","\n","# 単語リストを入力する場合は、analyzerにlambda x: xを渡す必要がある\n","vectorizer = CountVectorizer(analyzer=lambda x: x)\n","# 単語リストを渡して出現回数をカウント\n","vector = vectorizer.fit_transform(pd_word_list).toarray()\n","# 出力用に整形\n","pd.DataFrame(vector, columns=vectorizer.get_feature_names_out(), index=pd_word_list.index)"]},{"cell_type":"markdown","id":"b7d80e09","metadata":{},"source":["### Q: TF-IDF 特徴量の作成\n","#### Awesome"]},{"cell_type":"code","execution_count":null,"id":"a0852505","metadata":{},"outputs":[],"source":["import numpy as np\n","\n","(\n","    pd_aozora_token\n","    # （1） 品詞の抽出とストップワードの除去\n","    .query(\"pos in ('NOUN', 'PROPN') and not is_stop\")\n","    # （2） text_idごとにlemmaの値をカウント\n","    .groupby([\"text_id\", \"lemma\"]).size().rename(\"word_count\")\n","    # （3） TF-IDFを計算\n","    .to_frame()\n","    .reset_index()\n","    .assign(\n","        # （3）-1 文章ごとに全単語の出現回数の合計\n","        total_word_count=lambda df:\n","            df.groupby(\"text_id\").word_count.transform(\"sum\"),\n","        # （3）-2 単語ごとにその単語が含まれる文章数\n","        text_count_with_word=lambda df:\n","            df.groupby(\"lemma\").text_id.transform(\"nunique\"),\n","        # （3）-3 全文章数\n","        text_count=lambda df: df.text_id.nunique(),\n","        # （3）-4 TF=ある文章における対象単語の出現回数/ある文章に含まれる全単語の出現回数の合計\n","        tf=lambda df: df.word_count / df.total_word_count,\n","        # （3）-5 IDF=log(全文章数/対象単語が含まれる文章数) + 1\n","        idf=lambda df: np.log(df.text_count / df.text_count_with_word) + 1,\n","        # （3）-6 TF-IDF=TF*IDF\n","        tf_idf=lambda df: df.tf * df.idf,\n","        # （3）-7 TF-IDFを文章ごとに計算したL2ノルムで割って正規化\n","        tf_idf_normalized=lambda df:\n","            df.tf_idf / df.groupby(\"text_id\").tf_idf.transform(np.linalg.norm)\n","    )\n","    # （4） TF-IDFの列のみに絞り、横持ち行列に変換\n","    .set_index([\"text_id\", \"lemma\"])[\"tf_idf_normalized\"]\n","    .unstack()\n",")"]},{"cell_type":"code","execution_count":null,"id":"e9577fa6","metadata":{"lines_to_next_cell":0},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# text_idごとの単語リストを作成\n","pd_word_list = (\n","    pd_aozora_token\n","    .query(\"pos in ('NOUN', 'PROPN') and not is_stop\")\n","    # text_idごとにlemmaのlistを作成\n","    .groupby(\"text_id\").lemma.apply(list)\n",")\n","\n","# 単語リストを入力する場合は、analyzerにlambda x: xを渡す必要がある\n","# デフォルトではゼロ除算を防ぐためにsmooth_idf=Trueになるが、上のコードと若干数値がずれるためFalseに設定\n","vectorizer = TfidfVectorizer(analyzer=lambda x: x, smooth_idf=False)\n","# 単語リストを渡して出現回数をカウント\n","vector = vectorizer.fit_transform(pd_word_list).toarray()\n","# 出力用に整形\n","pd.DataFrame(vector, columns=vectorizer.get_feature_names_out(),\n","             index=pd_word_list.index)"]},{"cell_type":"markdown","id":"f8833820","metadata":{},"source":["### Q: 平均単語ベクトル特徴量の作成\n","#### Awesome"]},{"cell_type":"code","execution_count":null,"id":"5892a8f1","metadata":{},"outputs":[],"source":["(\n","    pd_aozora_token\n","    # （1） 品詞の抽出とストップワードの除去\n","    .query(\"pos in ('NOUN', 'PROPN') and not is_stop\")\n","    # （2） 作品ごとに単語ベクトルの平均を計算\n","    .groupby(\"text_id\").vector.mean()\n",")"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":2}
